//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33567101
// Cuda compilation tools, release 12.3, V12.3.107
// Based on NVVM 7.0.1
//

.version 8.3
.target sm_75
.address_size 64

	// .globl	__intersection__nn_2d
.weak .global .align 4 .b8 _ZZN4cuda3std3__48__detail21__stronger_order_cudaEiiE7__xform[16] = {3, 0, 0, 0, 4, 0, 0, 0, 4, 0, 0, 0, 3};
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust6system6detail10sequential3seqE[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust8cuda_cub3parE[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust8cuda_cub10par_nosyncE[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders2_1E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders2_2E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders2_3E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders2_4E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders2_5E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders2_6E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders2_7E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders2_8E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders2_9E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust12placeholders3_10E[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20436thrust3seqE[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20434cuda3std3__48in_placeE[1];
.global .align 1 .b8 __nv_static_38__75f8b74c_16_shaders_nn_2d_cu_40bf2043__ZN47_INTERNAL_75f8b74c_16_shaders_nn_2d_cu_40bf20434cuda3std6ranges3__45__cpo4swapE[1];
.extern .const .align 8 .b8 params[120];

.visible .entry __intersection__nn_2d()
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<22>;
	.reg .f64 	%fd<24>;
	.reg .b64 	%rd<22>;


	mov.u32 	%r3, 0;
	// begin inline asm
	call (%r2), _optix_get_payload, (%r3);
	// end inline asm
	mov.u32 	%r5, 1;
	// begin inline asm
	call (%r4), _optix_get_payload, (%r5);
	// end inline asm
	// begin inline asm
	call (%r6), _optix_read_primitive_idx, ();
	// end inline asm
	ld.const.u64 	%rd3, [params+40];
	cvta.to.global.u64 	%rd4, %rd3;
	mul.wide.u32 	%rd5, %r2, 16;
	add.s64 	%rd1, %rd4, %rd5;
	ld.const.u64 	%rd6, [params+56];
	cvta.to.global.u64 	%rd7, %rd6;
	mul.wide.u32 	%rd8, %r6, 16;
	add.s64 	%rd2, %rd7, %rd8;
	ld.const.f64 	%fd1, [params+88];
	ld.const.u64 	%rd9, [params+96];
	cvta.to.global.u64 	%rd10, %rd9;
	red.global.add.u32 	[%rd10], 1;
	ld.global.f64 	%fd3, [%rd2];
	sub.f64 	%fd4, %fd3, %fd1;
	ld.global.f64 	%fd5, [%rd1];
	setp.ltu.f64 	%p1, %fd5, %fd4;
	add.f64 	%fd6, %fd1, %fd3;
	setp.gtu.f64 	%p2, %fd5, %fd6;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB0_6;

	ld.global.f64 	%fd7, [%rd2+8];
	sub.f64 	%fd8, %fd7, %fd1;
	ld.global.f64 	%fd9, [%rd1+8];
	setp.ltu.f64 	%p4, %fd9, %fd8;
	add.f64 	%fd10, %fd1, %fd7;
	setp.gtu.f64 	%p5, %fd9, %fd10;
	or.pred  	%p6, %p4, %p5;
	@%p6 bra 	$L__BB0_6;

	ld.global.v2.f64 	{%fd11, %fd12}, [%rd1];
	ld.global.v2.f64 	{%fd15, %fd16}, [%rd2];
	sub.f64 	%fd19, %fd11, %fd15;
	sub.f64 	%fd20, %fd12, %fd16;
	mul.f64 	%fd21, %fd20, %fd20;
	fma.rn.f64 	%fd2, %fd19, %fd19, %fd21;
	mov.u32 	%r9, 2;
	// begin inline asm
	call (%r8), _optix_get_payload, (%r9);
	// end inline asm
	mov.u32 	%r11, 3;
	// begin inline asm
	call (%r10), _optix_get_payload, (%r11);
	// end inline asm
	cvt.u64.u32 	%rd11, %r8;
	cvt.u64.u32 	%rd12, %r10;
	bfi.b64 	%rd13, %rd11, %rd12, 32, 32;
	mov.b64 	%fd22, %rd13;
	setp.geu.f64 	%p7, %fd2, %fd22;
	@%p7 bra 	$L__BB0_4;

	mov.b64 	%rd14, %fd2;
	shr.u64 	%rd15, %rd14, 32;
	cvt.u32.u64 	%r13, %rd15;
	cvt.u32.u64 	%r15, %rd14;
	// begin inline asm
	call _optix_set_payload, (%r9, %r13);
	// end inline asm
	// begin inline asm
	call _optix_set_payload, (%r11, %r15);
	// end inline asm

$L__BB0_4:
	ld.const.u64 	%rd16, [params+80];
	cvta.to.global.u64 	%rd17, %rd16;
	ld.global.f64 	%fd23, [%rd17];
	add.s32 	%r17, %r4, 1;
	mov.u32 	%r16, 1;
	// begin inline asm
	call _optix_set_payload, (%r16, %r17);
	// end inline asm
	setp.gtu.f64 	%p8, %fd2, %fd23;
	@%p8 bra 	$L__BB0_6;

	ld.const.u64 	%rd18, [params+104];
	cvta.to.global.u64 	%rd19, %rd18;
	red.global.add.u32 	[%rd19], 1;
	ld.const.u64 	%rd20, [params+112];
	cvta.to.global.u64 	%rd21, %rd20;
	red.global.add.u32 	[%rd21], %r4;
	mov.f32 	%f1, 0f00000000;
	mov.u32 	%r19, 0;
	// begin inline asm
	call (%r18), _optix_report_intersection_0, (%f1, %r19);
	// end inline asm

$L__BB0_6:
	ret;

}
	// .globl	__anyhit__nn_2d
.visible .entry __anyhit__nn_2d()
{



	// begin inline asm
	call _optix_terminate_ray, ();
	// end inline asm
	ret;

}
	// .globl	__raygen__nn_2d
.visible .entry __raygen__nn_2d()
{
	.reg .pred 	%p<6>;
	.reg .f32 	%f<10>;
	.reg .b32 	%r<115>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<31>;


	// begin inline asm
	call (%r114), _optix_get_launch_index_x, ();
	// end inline asm
	cvt.u64.u32 	%rd29, %r114;
	ld.const.u64 	%rd2, [params+8];
	setp.le.u64 	%p1, %rd2, %rd29;
	@%p1 bra 	$L__BB2_8;

	ld.const.u64 	%rd15, [params];
	cvta.to.global.u64 	%rd3, %rd15;
	ld.const.u64 	%rd16, [params+40];
	cvta.to.global.u64 	%rd4, %rd16;
	ld.const.u64 	%rd5, [params+72];
	ld.const.u64 	%rd17, [params+80];
	cvta.to.global.u64 	%rd6, %rd17;
	ld.const.u64 	%rd18, [params+32];
	cvta.to.global.u64 	%rd7, %rd18;
	ld.const.u64 	%rd19, [params+16];
	cvta.to.global.u64 	%rd8, %rd19;
	// begin inline asm
	call (%r111), _optix_get_launch_dimension_x, ();
	// end inline asm

$L__BB2_2:
	shl.b64 	%rd21, %rd29, 2;
	add.s64 	%rd22, %rd3, %rd21;
	ld.global.u32 	%r78, [%rd22];
	mul.wide.u32 	%rd23, %r78, 16;
	add.s64 	%rd24, %rd4, %rd23;
	ld.global.v2.f64 	{%fd2, %fd3}, [%rd24];
	mov.u32 	%r109, 0;
	mov.u32 	%r75, 1;
	cvt.rn.f32.f64 	%f1, %fd2;
	cvt.rn.f32.f64 	%f2, %fd3;
	mov.f32 	%f6, 0f3F800000;
	mov.f32 	%f8, 0f00800000;
	mov.f32 	%f9, 0f00000000;
	mov.u32 	%r72, 255;
	mov.u32 	%r77, 4;
	mov.u32 	%r80, 2146435071;
	mov.u32 	%r81, -1;
	// begin inline asm
	call(%r39,%r40,%r41,%r42,%r43,%r44,%r45,%r46,%r47,%r48,%r49,%r50,%r51,%r52,%r53,%r54,%r55,%r56,%r57,%r58,%r59,%r60,%r61,%r62,%r63,%r64,%r65,%r66,%r67,%r68,%r69,%r70),_optix_trace_typed_32,(%r109,%rd5,%f1,%f2,%f9,%f9,%f9,%f6,%f9,%f8,%f9,%r72,%r109,%r109,%r75,%r109,%r77,%r78,%r109,%r80,%r81,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109,%r109);
	// end inline asm
	cvt.u64.u32 	%rd25, %r41;
	cvt.u64.u32 	%rd26, %r42;
	bfi.b64 	%rd10, %rd25, %rd26, 32, 32;
	mov.b64 	%fd1, %rd10;
	setp.neu.f64 	%p2, %fd1, 0d7FEFFFFFFFFFFFFF;
	@%p2 bra 	$L__BB2_4;
	bra.uni 	$L__BB2_3;

$L__BB2_4:
	ld.global.u64 	%rd30, [%rd6];

$L__BB2_5:
	mov.b64 	%fd6, %rd30;
	setp.geu.f64 	%p3, %fd6, %fd1;
	@%p3 bra 	$L__BB2_7;

	atom.global.cas.b64 	%rd13, [%rd6], %rd30, %rd10;
	setp.ne.s64 	%p4, %rd13, %rd30;
	mov.u64 	%rd30, %rd13;
	@%p4 bra 	$L__BB2_5;
	bra.uni 	$L__BB2_7;

$L__BB2_3:
	atom.global.add.u32 	%r110, [%rd7], 1;
	mul.wide.u32 	%rd27, %r110, 4;
	add.s64 	%rd28, %rd8, %rd27;
	st.global.u32 	[%rd28], %r39;

$L__BB2_7:
	add.s32 	%r114, %r111, %r114;
	cvt.u64.u32 	%rd29, %r114;
	setp.gt.u64 	%p5, %rd2, %rd29;
	@%p5 bra 	$L__BB2_2;

$L__BB2_8:
	ret;

}
	// .weak	_ZN3cub17CUB_200200_750_NS11EmptyKernelIvEEvv
.weak .entry _ZN3cub17CUB_200200_750_NS11EmptyKernelIvEEvv()
{



	ret;

}

